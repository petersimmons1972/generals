# Post #1: Tech-Honest + Human-Messy Hybrid

**New approach:** Say what they're building (AI agents, websites, multi-variant) but keep all the human conflict/mess/mistakes

**Hypothesis:** Maybe being honest about tech + messy humans = passes better than hiding tech

---

Montgomery looks terrible. It's 2:37 AM and he's explaining to twenty-four people how they're going to build fourteen different websites using AI agents with personalities based on World War II generals.

Half the room thinks this is brilliant. The other half thinks it's insane.

Patton's in the first half. Rickover's in the second.

"So we're assigning... personality traits... to code?" someone asks.

"To AI agents," Montgomery says. "Patton gets the aggressive variant. Nimitz gets the calm one. Rickover gets technical precision."

"That's anthropomorphizing software," Rickover says. Doesn't look up from his screen.

"That's engineering for predictability," Montgomery says.

They've been having this argument for three hours.

Here's what they're actually building: Fourteen websites. Same biographical content. But each one with a different design personality—brutal, calm, technical, elegant. They're testing which personality traits convert which visitors. Academic researchers might prefer documentation style. Security practitioners might want terminal aesthetics.

The AI agents building these sites aren't just tools. They're... I don't know how to describe it. Montgomery keeps calling them "commanders" like they're people. Patton ships fast and fixes bugs later. Rickover won't ship anything until it's perfect.

Except they're not people. They're AI.

But watching them work, you'd almost think—

Patton just broke something. His third rebuild tonight. Took him three hours to build it, ten minutes to break it, and he's already starting over.

Rickover's on his first version. Still. He deleted four hours of work around midnight because one section "didn't feel right."

"You can't feel code," Patton says.

"Then you're not paying attention," Rickover says.

Around 3 AM they get in a real argument. Patton's at Rickover's desk, pointing at his screen. "You're overthinking this. Ship it. Fix it later. That's how you learn what works."

"That's how you ship garbage."

"That's how you meet deadlines."

"Then maybe the deadline's wrong."

Montgomery appears between them. Doesn't say anything. Patton walks back to his desk. The argument's not resolved. They'll have it again tomorrow.

Bedell Smith—Montgomery's assistant—tells me this happens every deployment. "Patton thinks speed matters most. Rickover thinks quality is the only thing that matters. They're both right and both wrong."

"Who wins?"

"Whoever ships first."

Around 3:30 someone's computer crashes. Two hours of AI agent training data gone. Just... gone.

"Did you checkpoint it?" Montgomery asks.

Long pause. "Most of it."

You can feel the room tense up. This isn't just rewriting text. This is retraining an AI agent's decision patterns. That takes hours.

"Rebuild it," Montgomery says. "Everyone else, checkpoint now."

Five people haven't checkpointed all night. Montgomery makes them do it in front of everyone. It's brutal.

There's a woman who walks around testing the AI agents' outputs. Reading them out loud. Every time an agent generates something that sounds too robotic, too corporate, too AI, she shakes her head and someone has to retrain it.

"How many retrains tonight?" I ask Bedell Smith.

He thinks about it. "Thirty? Forty?"

"Is that normal?"

"For this? Yeah. You're training AI agents to write like humans with distinct personalities. Most of them default to corporate jargon. You have to break that."

Around 4 AM something goes wrong with King's agent. The outputs don't match the personality profile. King thinks his training data is right. Halsey thinks King's approach is wrong. Nimitz thinks they're both wrong.

It gets loud. Everyone's talking at once.

Montgomery walks over. Looks at the outputs. "King, your data is right but you're overfitting. Halsey, you're underfitting. Nimitz, show them the balance."

Nimitz adjusts something. King watches. Halsey watches.

"Oh," King says.

"Yeah," Nimitz says.

Problem's not solved. But at least they stopped yelling.

By 5 AM everyone's exhausted. Real exhausted. Making mistakes. Someone deletes the wrong training checkpoint. Someone else overwrites a working agent configuration. Someone's halfway through retraining an agent and just... stops. Stares at the screen. Closes their eyes.

Patton's looking at his outputs like they personally betrayed him. His agent keeps generating aggressive copy but it's TOO aggressive. Sounds angry, not confident.

"How do you train an AI to sound confident without sounding arrogant?" he asks nobody in particular.

Nobody answers because nobody knows.

Rickover's still on his first agent. Same one. Still training. Still adjusting. If he's tired I cannot tell.

Around 5:30 there's another argument about whether AI agents can actually have "personalities" or if this is just elaborate prompt engineering.

Montgomery doesn't even look up. Just says "ship something that works and we'll argue about what it means later."

They stop arguing. Whatever philosophical question they were debating doesn't get answered. They just go back to work.

Around 6 AM Montgomery says "done."

"The agents aren't trained yet," someone says.

"They're trained enough. Sleep. Back at 2 PM to see what they generated overnight."

"Overnight?"

"They'll keep training while you sleep. That's the point of agents."

People slowly pack up. Patton's agent has generated maybe three pages of content. Rickover's generated a thick stack but Rickover still won't call it done.

They shuffle out.

"Does this actually work?" I ask Montgomery. "Training AI agents to write like World War II generals?"

"We'll find out," he says. Turns off the lights. We walk out.

I go back to my hotel.

Can't stop thinking about the weirdness of it. Twenty-four people training AI agents to have personalities. Arguing about whether code can "feel" right. Debugging an agent's confidence level at 5 AM.

It's either brilliant or completely insane.

Maybe both.

I'll be back at 2 PM to see what the agents generated while everyone slept.

---

## WHAT'S DIFFERENT

**Put back:**
- ✅ AI agents explicitly mentioned
- ✅ Training, checkpointing, overfitting (real AI terms)
- ✅ What they're building (14 websites, personality variants)
- ✅ The actual technical challenge (training agents to write like humans)
- ✅ Honest about the weird concept

**Kept from conflict version:**
- ✅ Patton/Rickover arguments
- ✅ Real mistakes (computer crash loses data)
- ✅ People being wrong (King, Halsey, Nimitz)
- ✅ Exhaustion and frustration
- ✅ Philosophical argument that doesn't resolve
- ✅ Multiple conflicts
- ✅ Technical problems
- ✅ Messiness throughout

**The bet:** Being honest about AI content + human drama = passes better than hiding AI content.

**TEST THIS and see if technical honesty + human mess works.**
