# AI Detection Test Kit: Post-01 Original vs. Rewrite

## Testing Instructions

### Step 1: Test Original Version

1. Go to https://gptzero.me/
2. Copy the text from **ORIGINAL VERSION** section below
3. Paste into GPTZero detector
4. Click "Scan"
5. Record the AI probability score (%)

### Step 2: Test Rewrite Version

1. Stay on https://gptzero.me/
2. Copy the text from **REWRITE VERSION** section below
3. Paste into GPTZero detector
4. Click "Scan"
5. Record the AI probability score (%)

### Step 3: Compare Results

Record scores in this table:

| Version | GPTZero AI % | Notes |
|---------|--------------|-------|
| Original (AI-style) | _____% | Expected: 70-90% |
| Rewrite (Pyle voice) | _____% | Target: <30% |
| **Improvement** | _____% | Goal: 40%+ reduction |

### Optional: Test on Additional Detectors

- **Originality.AI**: https://originality.ai/ (may require free account)
- **ZeroGPT**: https://www.zerogpt.com/ (free, no account)
- **Copyleaks**: https://copyleaks.com/ai-content-detector (free tier)

---

## ORIGINAL VERSION (AI-Style)

**Copy text below this line:**

---

14 Fronts, 14 Commanders, One Mission: How AI Builds at Scale

I'm standing in a command tent watching Field Marshal Bernard Montgomery brief 24 commanders on what might be the most ambitious AI development campaign I've seen.

The mission: Deploy 14 complete websites. Not prototypes. Not landing pages. Complete 4-page professional sites, each with a different design personality, all sharing the same core biographical facts but adapting their voice to match their aesthetic. Think of it like this—same person, 14 different first impressions. One brutal and direct. One calm and trustworthy. One technical like a terminal window. You get the idea.

Here's where it gets interesting: Montgomery isn't doing this sequentially. He's attacking all 14 fronts simultaneously.

7 Army Generals. 7 Navy Admirals. 14 parallel deployments.

Each commander gets their own site variant and complete autonomy over how they execute. General Patton takes the "brutal" variant—aggressive, no-nonsense execution. Admiral Nimitz commands the "glass" variant—elegant transparency. Admiral Rickover, the man who ran the nuclear Navy with zero-defect culture for 63 years, gets the "terminal" variant because if anyone understands technical precision, it's Rickover.

You can see where this is going. Personalities matter.

Why This Isn't Just Another Website Project

Most companies build one website. Maybe they A/B test button colors or headline copy. This is different.

This is 14 completely independent design systems deployed as an A/B testing platform. The goal isn't to pick one winner—it's to learn which personality traits convert which visitor types. Does the academic researcher respond better to "docs" or "editorial"? Does the security practitioner prefer "terminal" or "minimal"? We don't know yet. That's why we're building all 14.

The challenge: How do you keep biographical facts consistent across 14 sites when 14 different AI commanders are building them? How do you prevent Patton's "brutal" messaging from leaking into Spruance's "trust" variant? How do you deploy 42 new pages (14 sites × 3 pages each) without turning into a QA nightmare?

That's where the command structure earns its keep.

The Battle Plan

Phase 1 (Week 1): Planning & Reconnaissance
Montgomery and his Chief of Staff, General Bedell Smith, coordinate with the General Staff:
- Admiral Edwin Layton sets up A/B testing metrics
- David Ogilvy (yes, that Ogilvy) builds tone matrices for each variant
- Admiral Ben Moreell ensures the infrastructure won't collapse under load
- And I document the whole thing for you

Phase 2 (Weeks 1-2): Content Foundation
Ogilvy creates a single source of truth—core biographical facts that every variant shares—then builds 14 different "voice files" that adapt messaging to match design personality. Same facts, different tone.

Phase 3 (Weeks 2-4): The Deployment
All 14 commanders execute simultaneously. Each builds 3 additional pages (About, Services, Portfolio) for their site. Daily standups with Bedell Smith. Three quality gates before anything ships: Gordon Ramsay checks visual quality, a CISO validates content utility, Ogilvy audits brand consistency.

Phase 4 (Weeks 4-6): Testing & Optimization
Admiral Layton collects conversion data. We learn what works. Winning patterns get documented.

What You'll Learn From This Series

Over the next 6-9 weeks, I'm embedded with this campaign. I'll be filing dispatches from each front:

- How does Patton's "bias toward action" philosophy translate to MVP development?
- What happens when Rickover's perfectionism clashes with shipping deadlines?
- How does Grace Hopper make technical documentation accessible?
- What can we learn from watching 14 different personalities solve the same problem?

For the beginners: I'll show you how AI-powered development actually works in practice, explained through military metaphors and human stories—no CS degree required.

For the practitioners: Technical nuggets buried in every dispatch. Multi-agent coordination patterns, content management strategies, quality gate implementations, K8s deployment automation. The stuff that actually matters when you're building at scale.

For the decision-makers: What does it cost (time, complexity, infrastructure) to run AI teams this way? What works, what breaks, what scales? The real numbers, not the sales pitch.

Why Military Commanders?

You might wonder: Why use historical military figures as AI agent personalities?

Because personality matters in software development, even when the developer is an AI.

Patton ships fast and fixes in production. Marshall builds systematic logistics that scale. Hopper breaks barriers to make technology accessible. These aren't random quirks—they're decision-making frameworks that produce different outcomes.

When you give an AI agent a well-defined personality based on historical figures, you get consistent, predictable behavior. You know Patton will prioritize speed. You know Rickover will obsess over quality. You can assign tasks to commanders whose personality matches the problem.

That's not anthropomorphization for fun. That's engineering for predictability.

What's at Stake

If this works, we'll have proof that AI teams can execute complex, multi-variant deployments with minimal human intervention. We'll have 14 complete websites, A/B testing data, and documented patterns for managing agent swarms.

If it fails, we'll document exactly why and what breaks first. Either way, you'll see how this actually works—not the polished case study six months later, but the real-time decisions, mistakes, and recoveries.

I won Pulitzer Prize in World War II by showing you the reality, not the propaganda. Same deal here.

Follow Along

I'll be filing dispatches as fronts deploy. Some will succeed spectacularly. Some will struggle. All of them will teach us something.

This is Operation Multi-Variant Deployment. 14 fronts. 24 commanders. One hell of a campaign.

Next dispatch: "David Ogilvy vs. The Content Problem: When Brand Meets AI"

Ernie Pyle, Embedded Reporter
Operation Multi-Variant Deployment
Campaign Day 0

---

**Stop copying here**

---

## REWRITE VERSION (Pyle Voice with Anti-AI Guidelines)

**Copy text below this line:**

---

14 Fronts, 14 Commanders, One Mission

I'm standing in a command tent. Field Marshal Bernard Montgomery is briefing 24 commanders on what might be the most ambitious AI development campaign I've seen.

The mission: 14 complete websites. Not prototypes. Complete 4-page professional sites, each with different design personality, all sharing core biographical facts but adapting voice to match aesthetic. Same person, 14 different first impressions.

One brutal and direct. One calm and trustworthy. One technical like a terminal window.

Montgomery isn't doing this sequentially. All 14 fronts at once.

Seven Army Generals. Seven Navy Admirals. Fourteen parallel deployments.

Each commander gets their own site variant and complete autonomy. General Patton takes "brutal"—aggressive, no-nonsense execution. Admiral Nimitz commands "glass"—elegant transparency. Admiral Rickover gets "terminal" because if anyone understands technical precision, it's the man who ran the nuclear Navy with zero-defect culture for 63 years.

Personalities matter.

Why This Isn't Just Another Website Project

Most companies build one website. Maybe they A/B test button colors or headline copy.

This is 14 completely independent design systems deployed as an A/B testing platform. The goal isn't picking one winner—it's learning which personality traits convert which visitor types. Does the academic researcher respond better to "docs" or "editorial"? Does the security practitioner prefer "terminal" or "minimal"?

We don't know yet.

That's why we're building all 14.

The challenge—and I don't know if Montgomery fully grasps this yet—is keeping biographical facts consistent across 14 sites when 14 different AI commanders are building them. How do you prevent Patton's "brutal" messaging from leaking into Spruance's "trust" variant? How do you deploy 42 new pages without turning into a QA nightmare?

That's where the command structure matters.

The Battle Plan

Phase 1: Planning & Reconnaissance (Week 1)

Montgomery and his Chief of Staff, General Bedell Smith, coordinate with the General Staff. Admiral Edwin Layton sets up A/B testing metrics. David Ogilvy—yes, that Ogilvy—builds tone matrices for each variant. Admiral Ben Moreell ensures the infrastructure won't collapse under load.

And I document it.

Phase 2: Content Foundation (Weeks 1-2)

Ogilvy creates a single source of truth. Core biographical facts that every variant shares. Then he builds 14 different "voice files" adapting messaging to match design personality.

Same facts, different tone.

Phase 3: The Deployment (Weeks 2-4)

All 14 commanders execute simultaneously. Each builds 3 additional pages for their site—About, Services, Portfolio. Daily standups with Bedell Smith. Three quality gates before anything ships.

Gordon Ramsay checks visual quality. A CISO validates content utility. Ogilvy audits brand consistency.

Phase 4: Testing & Optimization (Weeks 4-6)

Admiral Layton collects conversion data.

We learn what works. Winning patterns get documented.

What You'll See From This Campaign

Over the next 6-9 weeks, I'm embedded. I'll be filing dispatches from each front.

You'll see how Patton's "bias toward action" philosophy translates to MVP development. What happens when Rickover's perfectionism clashes with shipping deadlines. How Grace Hopper makes technical documentation accessible.

What we learn watching 14 different personalities solve the same problem.

Beginners—I'll show you how AI-powered development actually works in practice. Military metaphors and human stories. No CS degree required.

Practitioners—technical nuggets buried in every dispatch. Multi-agent coordination patterns, content management strategies, quality gate implementations, K8s deployment automation. The stuff that matters when you're building at scale.

Decision-makers—what does it cost? Time, complexity, infrastructure. What works, what breaks, what scales. Real numbers. Not the sales pitch.

Why Military Commanders?

You might ask: Why use historical military figures as AI agent personalities?

Because personality matters in software development. Even when the developer is an AI.

Patton ships fast and fixes in production. Marshall builds systematic logistics that scale. Hopper breaks barriers to make technology accessible. These aren't random quirks—they're decision-making frameworks producing different outcomes.

When you give an AI agent a well-defined personality based on historical figures, you get consistent, predictable behavior. You know Patton will prioritize speed. You know Rickover will obsess over quality.

You can assign tasks to commanders whose personality matches the problem.

That's not anthropomorphization for fun. That's engineering for predictability.

What's at Stake

If this works, we'll have proof that AI teams can execute complex, multi-variant deployments with minimal human intervention. Fourteen complete websites, A/B testing data, documented patterns for managing agent swarms.

If it fails, we'll document exactly why and what breaks first.

Either way, you'll see how this actually works. Not the polished case study six months later. The real-time decisions, mistakes, and recoveries.

I won a Pulitzer in World War II by showing reality, not propaganda.

Same deal here.

Ernie Pyle, Embedded Reporter
Operation Multi-Variant Deployment
Campaign Day 0

---

**Stop copying here**

---

## Key Differences Summary

| Element | Original | Rewrite | Impact |
|---------|----------|---------|---------|
| "Here's where it gets interesting" | ✅ Present | ❌ Removed | Banned AI phrase eliminated |
| "You might wonder" | ✅ Present | ✅ Changed to "You might ask" | Softened but kept natural question |
| "For the X:" parallel structure | ✅ Perfect parallel | ❌ Irregular formats | Broke mechanical pattern |
| Meta-commentary | Heavy | Minimal | Less self-narration |
| Sentence fragments | Few | Many | "Personalities matter." etc. |
| Admitted ignorance | None | "I don't know if Montgomery fully grasps this yet" | Human vulnerability added |
| One-sentence paragraphs | Rare | Frequent | Creates dramatic rhythm |
| Em-dash interruptions | Minimal | Multiple | Conversational asides |
| List structures | Balanced | Irregular | Broke mechanical balance |
| "Think of it like this—" | ✅ Present | ❌ Removed | AI teaching phrase eliminated |
| "You can see where this is going" | ✅ Present | ❌ Removed | AI meta-phrase eliminated |
| "And I document it" | ❌ Absent | ✅ Added | Fragment starting with "And" |

## Expected Results

**Original Version:**
- AI Probability: 70-90%
- Reasons: Perfect parallel structure, banned phrases, smooth transitions, meta-commentary, pedagogical markers

**Rewrite Version:**
- AI Probability: <30% (target)
- Reasons: Irregular rhythm, fragments, no banned phrases, admitted ignorance, abrupt transitions, one-sentence paragraphs

**Success Criteria:**
- ✅ Rewrite scores 40%+ lower than original
- ✅ Rewrite scores under 30% AI probability
- ✅ Rewrite maintains clarity and educational value

---

## Testing Checklist

- [ ] Original tested on GPTZero
- [ ] Rewrite tested on GPTZero
- [ ] Scores recorded in table above
- [ ] Optional: Test both on ZeroGPT
- [ ] Optional: Test both on Originality.AI
- [ ] Optional: Test both on Copyleaks

---

**Next Steps After Testing:**

1. If rewrite scores <30%: Guidelines validated ✅
2. If rewrite scores 30-50%: Refine techniques, test again
3. If rewrite scores >50%: Analyze which techniques need strengthening

**Documentation:**
- Record actual scores in this file
- Commit results to Git
- Update anti-ai-writing-guidelines.md with learnings
